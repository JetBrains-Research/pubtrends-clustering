{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search optimization of clustering\n",
    "\n",
    "This notebook contains analysis of papers clustering optimization.\n",
    "It contains the following clustering methods:\n",
    "* LDA (Latent Dirichlet Allocation)\n",
    "* Louvain communities detection algorithm, followed by merging tiny clusters\n",
    "* Hierarchical clustering of word2vec based embeddings for citation graph and texts\n",
    "* DBScan of embeddings, followed by merging tiny clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without extension\n",
    "OUTPUT_NAME = 'grid_search_2021_11_02'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score, v_measure_score\n",
    "\n",
    "from utils.io import load_analyzer, load_clustering, get_review_pmids\n",
    "from utils.preprocessing import preprocess_clustering, get_clustering_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze ground truth clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "partitions_overall = [] \n",
    "\n",
    "review_pmids = get_review_pmids()\n",
    "n_reviews = len(review_pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "ground_truth_clusters_df = pd.DataFrame(columns=['Pmid', 'Level', 'Clusters'], dtype=object)\n",
    "logger.info('Computing ground truth clustering features')\n",
    "for pmid in tqdm(review_pmids):\n",
    "    clustering = load_clustering(pmid)\n",
    "    analyzer = load_analyzer(pmid)\n",
    "    \n",
    "    # Pre-calculate all hierarchy levels before grid search to avoid re-calculation of clusterings\n",
    "    for level in range(1, get_clustering_level(clustering)):\n",
    "        clusters = preprocess_clustering(\n",
    "            clustering, level, include_box_sections=False, uniqueness_method='unique_only'\n",
    "        )\n",
    "        ground_truth_clusters_df.loc[len(ground_truth_clusters_df)] = (pmid, level, len(set(clusters.values())))\n",
    "display(ground_truth_clusters_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir results\n",
    "\n",
    "sns.histplot(data=ground_truth_clusters_df, x='Clusters', hue='Level', element='poly')\n",
    "plt.title('Ground truth clusters number')\n",
    "plt.savefig(f'results/{OUTPUT_NAME}_ground_truth_clusters.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "See `grid_search.py` file to launch parameters grid search in parallel with Celery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_v_score(labels_true, labels_pred, reg=0.01):\n",
    "    v_score = v_measure_score(labels_true, labels_pred)\n",
    "    n_clusters = len(set(labels_pred))\n",
    "    return v_score - reg * n_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [adjusted_mutual_info_score, reg_v_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(f'{OUTPUT_NAME}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract parameter columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = set([m.__name__ for m in metrics])\n",
    "param_columns = list(set(results_df.columns) - score_columns - set(['level', 'n_clusters', 'pmid']))\n",
    "print(param_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of clusters and adjusted mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='method', y='n_clusters', hue='method', data=results_df)\n",
    "plt.title('Mean clusters number')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Clusters')\n",
    "plt.savefig(f'results/{OUTPUT_NAME}_mean_clusters_number.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='method', y='adjusted_mutual_info_score', hue='level', data=results_df)\n",
    "plt.title('Mean adjusted mutual information')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('AMI')\n",
    "plt.savefig(f'results/{OUTPUT_NAME}_mean_adjusted_mutual_information.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df = results_df.sort_values('adjusted_mutual_info_score', ascending=False).drop_duplicates(['method', 'pmid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='method', y='n_clusters', hue='method', data=best_df)\n",
    "plt.title('Clusters number for best params')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Clusters')\n",
    "plt.savefig(f'results/{OUTPUT_NAME}_best_clusters_number.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='method', y='adjusted_mutual_info_score', hue='level', data=best_df)\n",
    "plt.title('Adjusted mutual information for best params')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('AMI')\n",
    "plt.savefig(f'results/{OUTPUT_NAME}_best_adjusted_mutual_information.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_parameter_sets_for_method(score_df, param_cols, method, target_col, n=5):\n",
    "    return score_df[score_df.method == method].groupby(param_cols)[[target_col, 'n_clusters']].mean().sort_values(by=target_col, \n",
    "                                                                                                                  ascending=False).head(n).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_mean_score_for_method(score_df, param_cols, method, target_col):\n",
    "    return score_df[score_df.method == method].groupby(param_cols)[target_col].mean().sort_values(ascending=False).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target_col = 'adjusted_mutual_info_score'\n",
    "\n",
    "tops  = []\n",
    "for method in results_df.method.unique():\n",
    "    top_score = get_top_mean_score_for_method(results_df, param_columns, method, target_col)\n",
    "    print(method, ':', target_col, top_score, '\\n')\n",
    "    top_params_df = get_top_parameter_sets_for_method(results_df, param_columns, method, target_col)\n",
    "    display(top_params_df)\n",
    "    scores_df = results_df[results_df.method == method].copy()\n",
    "    for i, row in top_params_df[param_columns].iterrows():\n",
    "        filters = [True] * len(scores_df)\n",
    "        for p in param_columns:\n",
    "            filters = np.logical_and(filters, scores_df[p] == row[p])\n",
    "        t = scores_df.loc[filters].copy()\n",
    "        t['method'] = method\n",
    "        t['top'] = i + 1\n",
    "        tops.append(t)\n",
    "\n",
    "top_df = pd.concat(tops)\n",
    "sns.boxplot(x='method', y='adjusted_mutual_info_score', hue='top', data=top_df)\n",
    "plt.title('Adjusted mutual information')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('AMI')\n",
    "plt.savefig(f'results/{OUTPUT_NAME}_top_adjusted_mutual_information.png')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_data = []\n",
    "for method in results_df.method.unique():\n",
    "    method_data = []\n",
    "    for metric in metrics:\n",
    "        top_score = get_top_mean_score_for_method(results_df, param_columns, method, metric.__name__)\n",
    "        method_data.append(top_score)\n",
    "    mean_score_data.append((method, *method_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [m.__name__ for m in metrics]\n",
    "mean_score_df = pd.DataFrame(mean_score_data, columns=['method', *metric_names])\n",
    "mean_score_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_df.to_csv(f'results/{OUTPUT_NAME}_mean_scores_per_method.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mean_score_df.plot.bar(x='method', y=metric_names)\n",
    "fig = p.get_figure()\n",
    "fig.savefig(f'results/{OUTPUT_NAME}_mean_scores_per_method.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameters visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "categories = ['similarity_bibliographic_coupling',\n",
    "              'similarity_cocitation',\n",
    "              'similarity_citation']\n",
    "\n",
    "fig = go.Figure()\n",
    "for method in results_df.method.unique():\n",
    "    t = get_top_parameter_sets_for_method(results_df, param_columns, method, target_col)\n",
    "    r = (t['similarity_bibliographic_coupling'].values[0],\n",
    "         t['similarity_cocitation'].values[0],\n",
    "         t['similarity_citation'].values[0])\n",
    "    if method !='lda':\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=r,\n",
    "            theta=categories,\n",
    "            fill='toself',\n",
    "            name=method\n",
    "        ))\n",
    "fig.update_layout(\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "      range=[0, 10]\n",
    "    )),\n",
    "  showlegend=False\n",
    ")\n",
    "fig.write_image(f'results/{OUTPUT_NAME}_params.png')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Scores for Different Clustering Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_parameter_sets_for_level_and_method(score_df, param_cols, level, method, target_col, n=5):\n",
    "    return score_df[(score_df.method == method) & (score_df.level == level)]\\\n",
    "        .groupby(param_cols)[[target_col, 'n_clusters']].mean().sort_values(by=target_col, \n",
    "                                                                            ascending=False).head(n).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_mean_score_for_level_and_method(score_df, param_cols, level, method, target_col):\n",
    "    return score_df[(score_df.method == method) & (score_df.level == level)]\\\n",
    "        .groupby(param_cols)[target_col].mean().sort_values(ascending=False).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_col = 'adjusted_mutual_info_score'\n",
    "\n",
    "for level in results_df.level.unique():\n",
    "    tops = []\n",
    "    print(f'LEVEL {level}')\n",
    "    for method in results_df.method.unique():\n",
    "        top_score = get_top_mean_score_for_level_and_method(results_df, param_columns, level, method, target_col)\n",
    "        print(method, ':', target_col, top_score, '\\n')\n",
    "        top_params_df = get_top_parameter_sets_for_level_and_method(results_df, param_columns, level, method, target_col)\n",
    "        display(top_params_df)\n",
    "        top_params_df.to_csv(f'results/{OUTPUT_NAME}_top_params_{method}_{level}.csv', index=False)\n",
    "        scores_df = results_df[(results_df.method == method) & (results_df.level == level)].copy()\n",
    "        for i, row in top_params_df[param_columns].iterrows():\n",
    "            filters = [True] * len(scores_df)\n",
    "            for p in param_columns:\n",
    "                filters = np.logical_and(filters, scores_df[p] == row[p])\n",
    "            t = scores_df.loc[filters].copy()\n",
    "            t['method'] = method\n",
    "            t['top'] = i + 1\n",
    "            tops.append(t)\n",
    "\n",
    "    top_df = pd.concat(tops)\n",
    "    sns.boxplot(x='method', y='adjusted_mutual_info_score', hue='top', data=top_df)\n",
    "    plt.title(f'Adjusted mutual information level {level}')\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('AMI')\n",
    "    plt.savefig(f'results/{OUTPUT_NAME}_level_{level}_top_adjusted_mutual_information.png')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_mean_score_data = []\n",
    "\n",
    "for level in results_df.level.unique():\n",
    "    for method in results_df.method.unique():\n",
    "        method_data = []\n",
    "        for metric in metrics:\n",
    "            top_score = get_top_mean_score_for_level_and_method(results_df, param_columns, level, method, metric.__name__)\n",
    "            method_data.append(top_score)\n",
    "        level_mean_score_data.append((level, method, *method_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [m.__name__ for m in metrics]\n",
    "level_mean_score_df = pd.DataFrame(level_mean_score_data, columns=['level', 'method', *metric_names])\n",
    "level_mean_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_mean_score_df.to_csv(f'results/{OUTPUT_NAME}_mean_scores_per_method_and_level.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in level_mean_score_df.level.unique():\n",
    "    p = level_mean_score_df[level_mean_score_df.level == level].plot.bar(x='method', y=metric_names, title=f'Level {level}')\n",
    "    fig = p.get_figure()\n",
    "    fig.savefig(f'results/{OUTPUT_NAME}_mean_scores_per_method_level_{level}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "categories = ['similarity_bibliographic_coupling',\n",
    "              'similarity_cocitation',\n",
    "              'similarity_citation']\n",
    "\n",
    "\n",
    "for level in results_df.level.unique():\n",
    "    fig = go.Figure()\n",
    "    print(f'LEVEL {level}')\n",
    "    for method in results_df.method.unique():\n",
    "        t = get_top_parameter_sets_for_level_and_method(results_df, param_columns, level, method, target_col)\n",
    "        r = (t['similarity_bibliographic_coupling'].values[0],\n",
    "             t['similarity_cocitation'].values[0],\n",
    "             t['similarity_citation'].values[0])\n",
    "        if method !='lda':\n",
    "            fig.add_trace(go.Scatterpolar(\n",
    "                r=r,\n",
    "                theta=categories,\n",
    "                fill='toself',\n",
    "                name=method\n",
    "            ))\n",
    "    fig.update_layout(\n",
    "      polar=dict(\n",
    "        radialaxis=dict(\n",
    "          visible=True,\n",
    "          range=[0, 10]\n",
    "        )),\n",
    "      showlegend=False\n",
    "    )\n",
    "    fig.write_image(f'results/{OUTPUT_NAME}_params_{level}.png')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Visualization - Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
